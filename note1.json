{"paragraphs":[{"text":"%md\n\n<h3>Machine Learning with Unstructured Text Data</h3>\n<p>\n<br><strong>Purpose:</strong>\n<br>Shows how to use unstructured text data, combined with structured data, to predict an outcome. In this example, I am using labeled data to build a model that predicts wine points/ratings, based on the wine description.\n<br>\n<br><strong>Input Dataset:</strong> \n<br>This dataset contains online reviews from Lowes and Home Depot\n<br>\n<br><strong>Output Result:</strong> \n<br>A model, which can be used to score new wine to asset the point value. \n<br>\n<br>NOTES:\n<br><a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">PySpark Documentation</a>\n<br><a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.feature\" target=\"_blank\">PySpark Mllib Feature Extraction</a>\n<br><a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.classification\" target=\"_blank\">PySpark Mllib Classification Algorithms</a>\n<br><a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.evaluation\" target=\"_blank\">PySpark Mllib Model Evaluation</a>\n</p>","dateUpdated":"2016-10-11T02:29:15+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952454_-2113775427","id":"20160730-221350_1732180176","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Machine Learning with Unstructured Text Data</h3>\n<p>\n<br><strong>Purpose:</strong>\n<br>Shows how to use unstructured text data, combined with structured data, to predict an outcome. In this example, I am using labeled data to build a model that predicts wine points/ratings, based on the wine description.\n<br>\n<br><strong>Input Dataset:</strong> \n<br>This dataset contains online reviews from Lowes and Home Depot\n<br>\n<br><strong>Output Result:</strong> \n<br>A model, which can be used to score new wine to asset the point value. \n<br>\n<br>NOTES:\n<br><a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">PySpark Documentation</a>\n<br><a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.feature\" target=\"_blank\">PySpark Mllib Feature Extraction</a>\n<br><a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.classification\" target=\"_blank\">PySpark Mllib Classification Algorithms</a>\n<br><a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.evaluation\" target=\"_blank\">PySpark Mllib Model Evaluation</a>\n</p>\n"},"dateCreated":"2016-10-11T12:49:12+0000","dateStarted":"2016-10-11T02:29:15+0000","dateFinished":"2016-10-11T02:29:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16093","focus":true},{"text":"%dep\nz.reset()\n\n// Add spark-csv package\nz.load(\"com.databricks:spark-csv_2.10:1.2.0\")\n\n// Then reset Spark Interpreter for this to work\n//\n// Could also run:\n//      bin/pyspark --packages com.databricks:spark-csv_2.10:1.0.3\n// Could also add:\n//      spark.jars.packages com.databricks:spark-csv_2.11:1.2.0 \n//      to spark-defaults.conf\n","dateUpdated":"2016-10-11T02:29:17+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952456_-2116083920","id":"20160729-150251_1768409828","result":{"code":"SUCCESS","type":"TEXT","msg":"DepInterpreter(%dep) deprecated. Remove dependencies and repositories through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@4756f21f\n"},"dateCreated":"2016-10-11T12:49:12+0000","dateStarted":"2016-10-11T02:29:17+0000","dateFinished":"2016-10-11T02:29:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16094","focus":true},{"title":"Import Dataset as PySpark Dataframe","text":"%pyspark\n\nfrom pyspark.sql.types import *\n\nrawdata = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('hdfs://sandbox.hortonworks.com/tmp/ca_lowes.csv')\n\nrawdata.show(5)\n#sqlContext.registerDataFrameAsTable(rawdata, \"lowes\")\n\n###########################################################################################################################\n#\n#   Modify Datatypes\n#\n###########################################################################################################################\nprint rawdata.dtypes\n\nrecords = rawdata.withColumn(\"Rating\", rawdata[\"Rating\"].cast(DoubleType()))\n\nprint records.dtypes","dateUpdated":"2016-10-11T02:35:35+0000","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952457_-2116468669","id":"20160729-144845_714693202","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+-------+--------------------+--------+------+---------------+-----+--------------------+\n|Company|   Date|            Datetime|  Author|Rating|           City|State|             Comment|\n+-------+-------+--------------------+--------+------+---------------+-----+--------------------+\n|  Lowes|9/17/01|2001-09-17 21:57:...|   Aaron|     3|          Azusa|   CA|Paid full balance...|\n|  Lowes|3/11/02|2002-03-11 13:03:...|    Earl|     5|        Cumming|   GA|I submitted a pay...|\n|  Lowes| 8/2/04|2004-08-02 04:24:...|Brandace|     2| Upper Marlboro|   MD|This project star...|\n|  Lowes|7/23/04|2004-07-23 09:24:...|   James|     4|      Fallbrook|   CA|During the week o...|\n|  Lowes|7/13/04|2004-07-13 16:06:...|   Bruce|     1|Incline Village|   NV|On July 5  2003 I...|\n+-------+-------+--------------------+--------+------+---------------+-----+--------------------+\nonly showing top 5 rows\n\n[('Company', 'string'), ('Date', 'string'), ('Datetime', 'string'), ('Author', 'string'), ('Rating', 'string'), ('City', 'string'), ('State', 'string'), ('Comment', 'string')]\n[('Company', 'string'), ('Date', 'string'), ('Datetime', 'string'), ('Author', 'string'), ('Rating', 'double'), ('City', 'string'), ('State', 'string'), ('Comment', 'string')]\n"},"dateCreated":"2016-10-11T12:49:12+0000","dateStarted":"2016-10-11T02:35:35+0000","dateFinished":"2016-10-11T02:35:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16095","focus":true},{"text":"%pyspark\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import udf\n\ndef cleanup_text(text):\n    words = text.split(' ')\n    \n    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', u'can', 'cant', 'come', u'could', 'couldnt', u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', u'each', u'few', 'finally', u'for', u'from', u'further', u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', u'just', u'll', u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', u'no', u'nor', u'not', u'now', u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', u're', u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', u'under', u'until', u'up', u'very', u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n    \n    stopwords_custom = ['']\n    stopwords = stopwords_core + stopwords_custom\n    stopwords = [word.lower() for word in stopwords]    \n    \n    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       # Remove special characters\n    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]     # Remove stopwords and words under X length\n    \n    return text_out\n\nudf_cleantext = udf(cleanup_text , ArrayType(StringType()))\nclean_text = records.select(udf_cleantext(records.Comment).alias('words'))\n\nfor i in clean_text.take(1): print i","dateUpdated":"2016-10-11T02:35:37+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952458_-2115314423","id":"20160729-192528_1918967919","result":{"code":"SUCCESS","type":"TEXT","msg":"Row(words=[u'paid', u'full', u'balance', u'lowes', u'bill', u'june152001', u'received', u'bill', u'july', u'stated', u'payment', u'received', u'pay', u'online', u'thru', u'bank', u'washington', u'mutual', u'faxed', u'cancelled', u'check', u'far', u'three', u'different', u'times', u'monogram', u'credit', u'card', u'bank', u'71401', u'82001', u'83101', u'told', u'faxed', u'copy', u'received', u'take', u'714', u'working', u'days', u'82501', u'hung', u'told', u'fax', u'another', u'copy', u'cancelled', u'check', u'recently', u'sent', u'letter', u'collections', u'also', u'asking', u'copy', u'cancelled', u'check', u'seems', u'end', u'nightmare'])\n"},"dateCreated":"2016-10-11T12:49:12+0000","dateStarted":"2016-10-11T02:35:37+0000","dateFinished":"2016-10-11T02:35:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16096","focus":true},{"title":"Create TFIDF","text":"%pyspark\n\nfrom pyspark.ml.feature import HashingTF, IDF\n\nztf    = HashingTF(numFeatures=10, inputCol=\"words\", outputCol=\"features\").transform(clean_text)\nzidf   = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf_output\").fit(ztf)\nztfidf = zidf.transform(ztf)\n\nztfidf.show(5)","dateUpdated":"2016-10-11T02:35:41+0000","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952458_-2115314423","id":"20160729-194037_1569608542","result":{"code":"SUCCESS","type":"TEXT","msg":"+--------------------+--------------------+--------------------+\n|               words|            features|          idf_output|\n+--------------------+--------------------+--------------------+\n|[paid, full, bala...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n|[submitted, payme...|(10,[0,2,3,4,5,6,...|(10,[0,2,3,4,5,6,...|\n|[project, started...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n|[week, april, cal...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n|[july, 2003, char...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"},"dateCreated":"2016-10-11T12:49:12+0000","dateStarted":"2016-10-11T02:35:41+0000","dateFinished":"2016-10-11T02:35:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16097","focus":true},{"title":"Analytical Data Prep  (Feature Extraction, Train/Test Dataset Partition)","text":"%pyspark\n\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.sql.functions import monotonicallyIncreasingId\n\n###########################################################################################################################\n#\n#   \n#\n###########################################################################################################################\n\n# Create labeled data by using a User-Defined Function\nlabels   = records.select('Rating')\nfeatures = ztfidf.select(\"idf_output\")\n\nprint labels.count()\nprint features.count()\n\n# Create index for each row to join on\nlabels = labels.select(monotonicallyIncreasingId().alias(\"rowId\"),\"*\")\nfeatures = features.select(monotonicallyIncreasingId().alias(\"rowId\"),\"*\")\n\nlabels.show(5)\nfeatures.show(5)\n\nfinal_ds = labels.join(features, \"rowId\")\nfinal_ds.show(5)\n\ntraindata, testdata = final_ds.randomSplit([0.8, 0.2])","dateUpdated":"2016-10-11T02:35:46+0000","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952459_-2115699171","id":"20160729-200016_2018297436","result":{"code":"SUCCESS","type":"TEXT","msg":"2640\n2640\n+-----+------+\n|rowId|Rating|\n+-----+------+\n|    0|   3.0|\n|    1|   5.0|\n|    2|   2.0|\n|    3|   4.0|\n|    4|   1.0|\n+-----+------+\nonly showing top 5 rows\n\n+-----+--------------------+\n|rowId|          idf_output|\n+-----+--------------------+\n|    0|(10,[0,1,2,3,4,5,...|\n|    1|(10,[0,2,3,4,5,6,...|\n|    2|(10,[0,1,2,3,4,5,...|\n|    3|(10,[0,1,2,3,4,5,...|\n|    4|(10,[0,1,2,3,4,5,...|\n+-----+--------------------+\nonly showing top 5 rows\n\n+-----+------+--------------------+\n|rowId|Rating|          idf_output|\n+-----+------+--------------------+\n|   31|   3.0|(10,[0,1,2,3,4,5,...|\n|  231|   3.0|(10,[0,1,2,3,4,5,...|\n|  431|   1.0|(10,[0,1,2,3,4,5,...|\n|  631|   2.0|(10,[0,1,2,3,4,5,...|\n|  831|   2.0|(10,[0,1,2,3,4,5,...|\n+-----+------+--------------------+\nonly showing top 5 rows\n\n"},"dateCreated":"2016-10-11T12:49:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16098","dateFinished":"2016-10-11T02:35:53+0000","dateStarted":"2016-10-11T02:35:46+0000","focus":true},{"title":"Decision Tree","text":"%pyspark\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.feature import StringIndexer\n\n###########################################################################################################################################\n#\n#   Decision Tree (Regression)\n#\n###########################################################################################################################################\n\ndt_regression = DecisionTreeRegressor(featuresCol=\"idf_output\", labelCol=\"Rating\", predictionCol=\"prediction\", maxDepth=5)\ndt_regression_model = dt_regression.fit(final_ds)\nprint 'Model Depth:\\t' + str(dt_regression_model.depth)\nprint 'Number of Nodes:\\t' + str(dt_regression_model.numNodes)\ntesting_dataset = final_ds.where(final_ds.rowId <= 10)\ndt_regression_model.transform(testing_dataset).show()\n\n\n\n###########################################################################################################################################\n#\n#   Decision Tree (Classifier)\n#\n###########################################################################################################################################\n\n# The ML Decision Tree Classifier requires an index map, which converts categorical and numeric labels into a numeric index (as float):\nstringIndexer       = StringIndexer(inputCol=\"Rating\", outputCol=\"indexed\")\ndt_si_model         = stringIndexer.fit(final_ds)\ndt_si_model_ds      = dt_si_model.transform(final_ds)\nindex_to_label_map  = dt_si_model_ds.select(['Rating','indexed']).distinct()\n\n# Create the decision tree classifer based on that newly created index:\ndt_classifier       = DecisionTreeClassifier(featuresCol=\"idf_output\", labelCol=\"indexed\", maxDepth=5, maxBins=32, minInstancesPerNode=10)\ndt_classifier_model = dt_classifier.fit(dt_si_model_ds)\nprint 'Model Depth:\\t\\t' + str(dt_classifier_model.depth)\nprint 'Number of Nodes:\\t' + str(dt_classifier_model.numNodes)\ntesting_dataset = final_ds.where(final_ds.rowId <= 10)\nresults = dt_classifier_model.transform(testing_dataset)\n#results.show(20)\nresults.join(index_to_label_map, results.prediction == index_to_label_map.indexed).show(20)","dateUpdated":"2016-10-11T02:38:14+0000","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952459_-2115699171","id":"20160730-133331_753255446","result":{"code":"SUCCESS","type":"TEXT","msg":"Model Depth:\t5\nNumber of Nodes:\t45\n+-----+------+--------------------+------------------+\n|rowId|Rating|          idf_output|        prediction|\n+-----+------+--------------------+------------------+\n|    0|   3.0|(10,[0,1,2,3,4,5,...|1.9817927170868348|\n|    1|   5.0|(10,[0,2,3,4,5,6,...|               3.0|\n|    2|   2.0|(10,[0,1,2,3,4,5,...| 2.160756501182033|\n|    3|   4.0|(10,[0,1,2,3,4,5,...| 2.160756501182033|\n|    4|   1.0|(10,[0,1,2,3,4,5,...|1.9817927170868348|\n|    5|   2.0|(10,[0,1,2,3,4,5,...|1.8476190476190477|\n|    6|   3.0|(10,[0,2,3,4,5,6,...| 2.238709677419355|\n|    7|   2.0|(10,[0,1,2,3,4,5,...|2.0641025641025643|\n|    8|   1.0|(10,[0,1,2,4,5,6,...|2.0641025641025643|\n|    9|   1.0|(10,[0,1,2,3,4,5,...|2.3916083916083917|\n|   10|   2.0|(10,[0,1,2,3,4,5,...|2.0641025641025643|\n+-----+------+--------------------+------------------+\n\nModel Depth:\t\t5\nNumber of Nodes:\t55\n+-----+------+--------------------+--------------------+--------------------+----------+------+-------+\n|rowId|Rating|          idf_output|       rawPrediction|         probability|prediction|Rating|indexed|\n+-----+------+--------------------+--------------------+--------------------+----------+------+-------+\n|    1|   5.0|(10,[0,2,3,4,5,6,...|[59.0,83.0,53.0,1...|[0.26818181818181...|       1.0|   1.0|    1.0|\n|    4|   1.0|(10,[0,1,2,3,4,5,...|[82.0,131.0,77.0,...|[0.25076452599388...|       1.0|   1.0|    1.0|\n|    5|   2.0|(10,[0,1,2,3,4,5,...|[82.0,131.0,77.0,...|[0.25076452599388...|       1.0|   1.0|    1.0|\n|    7|   2.0|(10,[0,1,2,3,4,5,...|[59.0,83.0,53.0,1...|[0.26818181818181...|       1.0|   1.0|    1.0|\n|    8|   1.0|(10,[0,1,2,4,5,6,...|[20.0,25.0,6.0,2....|[0.37735849056603...|       1.0|   1.0|    1.0|\n|    9|   1.0|(10,[0,1,2,3,4,5,...|[59.0,83.0,53.0,1...|[0.26818181818181...|       1.0|   1.0|    1.0|\n|   10|   2.0|(10,[0,1,2,3,4,5,...|[4.0,14.0,1.0,0.0...|[0.2,0.7,0.05,0.0...|       1.0|   1.0|    1.0|\n|    0|   3.0|(10,[0,1,2,3,4,5,...|[12.0,11.0,5.0,0....|[0.41379310344827...|       0.0|   2.0|    0.0|\n|    2|   2.0|(10,[0,1,2,3,4,5,...|[203.0,161.0,116....|[0.38157894736842...|       0.0|   2.0|    0.0|\n|    3|   4.0|(10,[0,1,2,3,4,5,...|[203.0,161.0,116....|[0.38157894736842...|       0.0|   2.0|    0.0|\n|    6|   3.0|(10,[0,2,3,4,5,6,...|[9.0,2.0,11.0,0.0...|[0.40909090909090...|       2.0|   3.0|    2.0|\n+-----+------+--------------------+--------------------+--------------------+----------+------+-------+\n\n"},"dateCreated":"2016-10-11T12:49:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16099","dateFinished":"2016-10-11T02:39:13+0000","dateStarted":"2016-10-11T02:38:14+0000","focus":true},{"title":"Random Forest (Classifier)","text":"%pyspark\n\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer\n\n###########################################################################################################################################\n#\n#   Random Forest (Classifier)\n#\n###########################################################################################################################################\n\n# The ML Random Forest Classifier requires an index map, which converts categorical and numeric labels into a numeric index (as float):\nstringIndexer       = StringIndexer(inputCol=\"Rating\", outputCol=\"indexed\")\nrf_si_model         = stringIndexer.fit(final_ds)\nrf_si_model_ds      = rf_si_model.transform(final_ds)\nindex_to_label_map  = rf_si_model_ds.select(['Rating','indexed']).distinct()\n\n# Create the decision tree classifer based on that newly created index:\nrf_classifier       = RandomForestClassifier(featuresCol=\"idf_output\", labelCol=\"indexed\", predictionCol=\"prediction\", numTrees=20, maxDepth=10, seed=42)\nrf_classifier_model = rf_classifier.fit(rf_si_model_ds)\ntesting_dataset = final_ds.where(final_ds.rowId <= 10)\nresults = rf_classifier_model.transform(testing_dataset)\n#results.show(20)\nresults.join(index_to_label_map, results.prediction == index_to_label_map.indexed).show(20)","dateUpdated":"2016-10-11T02:41:41+0000","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952460_-2117622916","id":"20160730-193927_214553299","result":{"code":"SUCCESS","type":"TEXT","msg":"+-----+------+--------------------+--------------------+--------------------+----------+------+-------+\n|rowId|Rating|          idf_output|       rawPrediction|         probability|prediction|Rating|indexed|\n+-----+------+--------------------+--------------------+--------------------+----------+------+-------+\n|    1|   5.0|(10,[0,2,3,4,5,6,...|[5.52356637630415...|[0.27617831881520...|       1.0|   1.0|    1.0|\n|    4|   1.0|(10,[0,1,2,3,4,5,...|[4.18229689226869...|[0.20911484461343...|       1.0|   1.0|    1.0|\n|    5|   2.0|(10,[0,1,2,3,4,5,...|[6.26622473595396...|[0.31331123679769...|       1.0|   1.0|    1.0|\n|    8|   1.0|(10,[0,1,2,4,5,6,...|[3.90685697812348...|[0.19534284890617...|       1.0|   1.0|    1.0|\n|    9|   1.0|(10,[0,1,2,3,4,5,...|[4.32101130534839...|[0.21605056526741...|       1.0|   1.0|    1.0|\n|    3|   4.0|(10,[0,1,2,3,4,5,...|[2.85605835010332...|[0.14280291750516...|       3.0|   4.0|    3.0|\n|    2|   2.0|(10,[0,1,2,3,4,5,...|[11.0862715828640...|[0.55431357914320...|       0.0|   2.0|    0.0|\n|    7|   2.0|(10,[0,1,2,3,4,5,...|[9.97606312206696...|[0.49880315610334...|       0.0|   2.0|    0.0|\n|   10|   2.0|(10,[0,1,2,3,4,5,...|[13.1154032508510...|[0.65577016254255...|       0.0|   2.0|    0.0|\n|    0|   3.0|(10,[0,1,2,3,4,5,...|[5.61988535439357...|[0.28099426771967...|       2.0|   3.0|    2.0|\n|    6|   3.0|(10,[0,2,3,4,5,6,...|[4.57568063054883...|[0.22878403152744...|       2.0|   3.0|    2.0|\n+-----+------+--------------------+--------------------+--------------------+----------+------+-------+\n\n"},"dateCreated":"2016-10-11T12:49:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16100","dateFinished":"2016-10-11T02:44:07+0000","dateStarted":"2016-10-11T02:41:41+0000","focus":true},{"title":"Gradient Boosting","text":"%pyspark \n\nfrom pyspark.ml.regression import GBTRegressor\n\n###########################################################################################################################################\n#\n#   Gradient Boosting (Regressor)\n#\n###########################################################################################################################################\n\ngb          = GBTRegressor(featuresCol=\"idf_output\", labelCol=\"Rating\", predictionCol=\"prediction\", maxIter=15, maxDepth=10)\ngb_model    = gb.fit(final_ds)\ngb_model.transform(testdata).show()","dateUpdated":"2016-10-11T02:44:55+0000","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952460_-2117622916","id":"20160730-204318_911833964","result":{"code":"SUCCESS","type":"TEXT","msg":"+----------+------+--------------------+------------------+\n|     rowId|Rating|          idf_output|        prediction|\n+----------+------+--------------------+------------------+\n|8589935421|   2.0|(10,[0,1,2,3,4,5,...|1.9545210687254067|\n|       632|   3.0|(10,[0,1,2,3,4,5,...|2.5926363681636557|\n|      1032|   2.0|(10,[0,1,2,3,4,5,...| 1.934496749097907|\n|       233|   2.0|(10,[0,1,2,3,4,5,...| 1.971125895702773|\n|      1033|   2.0|(10,[0,1,2,3,4,5,...|2.2855164158950902|\n|8589935027|   1.0|(10,[0,1,2,3,4,5,...|1.4416430901617407|\n|       434|   3.0|(10,[0,1,2,3,4,5,...|2.7374525888800223|\n|      1234|   2.0|(10,[0,1,2,3,4,5,...| 2.136589281433462|\n|8589934624|   3.0|(10,[0,1,2,3,4,5,...|2.9464665561602628|\n|8589935224|   3.0|(10,[0,1,2,3,4,5,...|2.4527877884696347|\n|8589935424|   3.0|(10,[0,1,2,3,4,5,...|  2.62659817213162|\n|8589935624|   2.0|(10,[0,1,2,3,4,5,...|1.9227918640757027|\n|8589934625|   1.0|(10,[0,1,2,3,4,5,...|1.2053698525913854|\n|8589935825|   1.0|(10,[0,1,2,3,4,5,...|1.6644821565539762|\n|      1237|   2.0|(10,[0,1,2,4,5,6,...| 1.903033717222682|\n|       438|   1.0|(10,[0,1,2,3,5,6,...|1.3532054635504553|\n|      1038|   3.0|(10,[0,1,2,3,4,5,...| 3.025824420931137|\n|8589935228|   3.0|(10,[0,1,2,3,4,5,...|2.0335208154942914|\n|        39|   2.0|(10,[0,1,2,3,4,5,...| 1.731061812748965|\n|       239|   1.0|(10,[0,1,2,3,4,5,...|1.3738231939526084|\n+----------+------+--------------------+------------------+\nonly showing top 20 rows\n\n"},"dateCreated":"2016-10-11T12:49:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16101","dateFinished":"2016-10-11T02:55:40+0000","dateStarted":"2016-10-11T02:44:55+0000","focus":true},{"text":"%pyspark\n\n","dateUpdated":"2016-10-11T12:49:12+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476146952461_-2118007665","id":"20160801-122117_10939842","dateCreated":"2016-10-11T12:49:12+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16102"}],"name":"lowes_analysis","id":"2BXPXKDCD","angularObjects":{"2BW4S5NJ4:shared_process":[],"2BY1Y2HJG:shared_process":[],"2BVYH21ZJ:shared_process":[],"2BV6637D6:shared_process":[],"2BX5CPDSX:shared_process":[],"2BWHGGFZJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}
